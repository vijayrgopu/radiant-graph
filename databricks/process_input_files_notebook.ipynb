{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf96c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook: Process input CSV files, separate valid/invalid records, write valid to Delta table, log failed records\n",
    "\"\"\"\n",
    "This notebook processes input CSV files from S3, validates required columns, writes valid records to a Delta table partitioned by client/date/zip5, and writes failed records to a separate S3 location. Failed record counts are logged for each client.\n",
    "\"\"\"\n",
    "\n",
    "# Import required libraries\n",
    "import dbutils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, col, lit, udf, sha2, substring, regexp_replace\n",
    "from datetime import datetime\n",
    "import re\n",
    "import logging\n",
    "from pyspark.sql.types import StringType\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Get file path from Databricks widget\n",
    "dbutils.widgets.text(\"filepath\", \"\", \"S3 File Path\")\n",
    "filepath = dbutils.widgets.get(\"filepath\")\n",
    "\n",
    "# Set output locations and date\n",
    "today = datetime.utcnow().strftime(\"%Y/%m/%d\")\n",
    "processed_bucket = \"s3a://radiant-graph-delta-table/customer_data_by_client_date_zip\"\n",
    "failed_bucket = \"s3a://radiant-graph-input-failed\"\n",
    "\n",
    "# Set up logger for failed record counts\n",
    "logger = logging.getLogger(\"FailedRecordLogger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "try:\n",
    "    # Create Spark session with Delta support\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ProcessInputFiles\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read input CSV file from S3\n",
    "    df = spark.read.csv(filepath, header=True, inferSchema=True)\\\n",
    "        .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "    # Extract client name from file path using regex\n",
    "    def extract_client_name(path):\n",
    "        match = re.search(r\"client/([^/]+)/\", path)\n",
    "        return match.group(1) if match else \"unknown\"\n",
    "\n",
    "    extract_client_name_udf = udf(extract_client_name, StringType())\n",
    "    df = df.withColumn(\"client_name\", extract_client_name_udf(col(\"source_file\")))\n",
    "    df = df.withColumn(\"date\", lit(today))\n",
    "\n",
    "    # Define required columns for validation\n",
    "    required_columns = [\n",
    "        \"member_id\", \"first_name\", \"last_name\", \"dob\", \"gender\", \"phone\", \"email\", \"zip5\", \"plan_id\"\n",
    "     ]\n",
    "\n",
    "    # Separate invalid records (missing required columns)\n",
    "    invalid_df = df.filter(\n",
    "        \" OR \".join([f\"{col} IS NULL\" for col in required_columns])\n",
    "    )\n",
    "    valid_df = df.subtract(invalid_df)\n",
    "\n",
    "    # Compliance transformation for valid records\n",
    "    compliant_df = valid_df.select(\n",
    "        sha2(col(\"member_id\"), 256).alias(\"member_id_hash\"),\n",
    "        lit(\"REDACTED\").alias(\"first_name\"),\n",
    "        lit(\"REDACTED\").alias(\"last_name\"),\n",
    "        substring(col(\"dob\"), 1, 4).alias(\"birth_year\"),\n",
    "        col(\"gender\"),\n",
    "        regexp_replace(col(\"phone\"), r\"\\d{3}-\\d{3}\", \"***-***\").alias(\"phone_masked\"),\n",
    "        regexp_replace(col(\"email\"), r\"[^@]+\", \"user\").alias(\"email_masked\"),\n",
    "        substring(col(\"zip5\"), 1, 3).alias(\"zip3\"),\n",
    "        col(\"plan_id\"),\n",
    "        col(\"client_name\"),\n",
    "        col(\"date\"),\n",
    "        col(\"zip5\")\n",
    "    )\n",
    "\n",
    "    # Write compliant records to Delta table, partitioned and compressed\n",
    "    compliant_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"compression\", \"zstd\") \\\n",
    "        .partitionBy(\"client_name\", \"date\", \"zip5\") \\\n",
    "        .save(processed_bucket)\n",
    "\n",
    "    # Write failed records to S3 and log counts per client\n",
    "    total_records = df.count()\n",
    "    for client in invalid_df.select(\"client_name\").distinct().rdd.flatMap(lambda x: x).collect():\n",
    "        client_failed_df = invalid_df.filter(f\"client_name = '{client}'\")\n",
    "        failed_count = client_failed_df.count()\n",
    "        if failed_count > 0:\n",
    "            client_failed_df.write.mode(\"overwrite\").csv(\n",
    "                f\"{failed_bucket}/client/{client}/{today}/failed_records.csv\",\n",
    "                header=True\n",
    "            )\n",
    "            logger.info(f\"Client: {client}, Date: {today}, Failed Records: {failed_count}, Total Records Processed: {total_records}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    logger.error(f\"ERROR: Data ingestion failed for file {filepath}: {str(e)}\")\n",
    "    # Trigger SNS notification\n",
    "    try:\n",
    "        sns_topic_arn = os.environ.get(\"SNS_TOPIC_ARN\")\n",
    "        sns_client = boto3.client(\"sns\")\n",
    "        message = f\"ERROR: Data ingestion failed for file {filepath}: {str(e)}\"\n",
    "        subject = \"Radiant Graph Data Ingestion Failure\"\n",
    "        if sns_topic_arn:\n",
    "            sns_client.publish(TopicArn=sns_topic_arn, Message=message, Subject=subject)\n",
    "    except Exception as sns_e:\n",
    "        logger.error(f\"ERROR: Failed to send SNS notification: {str(sns_e)}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
